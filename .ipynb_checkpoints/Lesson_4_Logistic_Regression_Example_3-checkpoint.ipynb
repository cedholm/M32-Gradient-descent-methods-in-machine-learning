{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22f8a26",
   "metadata": {
    "id": "a02969f3"
   },
   "source": [
    "# Gradient Descent with Logistic Regression  - Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc6838",
   "metadata": {
    "id": "3c9dc99b"
   },
   "source": [
    "## Predicting admission to graduate school"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd3c24",
   "metadata": {
    "id": "951d4499"
   },
   "source": [
    "Now, let us put the gradient to work. We will use gradient descent to implement logistic regression in order to solve a classification problem.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "Our mission is to create a model that will estimate the probability of being admitted to a graduate program based on GRE scores and GPA.  We have a list of scores for $N$ students (the inputs, ${\\bf x}^{i}$), and we know whether or not they were admitted (the outputs, $y^{i}$, where a value of \"1\" means \"admitted\", and \"0\" means \"not admitted\").\n",
    "\n",
    "The data has three columns, where the first column consists of the outputs $y^{i}$ and the following columns consist of the inputs, ${\\bf x}^{i}$.  The second column gives all GRE test scores, and the third column contains the GPA.\n",
    "Before using Gradient Descent, we will plot the data using different symbols to represent the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778c543",
   "metadata": {
    "id": "50f2bf80",
    "outputId": "adb65564-9d38-4211-9741-38197cc8b48a"
   },
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Import packages\n",
    "#------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Plot style\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "#------------------------\n",
    "# Make dataframe\n",
    "#------------------------\n",
    "\n",
    "# Creates a dataframe from file called Admission-Data-Rank.csv\n",
    "#------------------------\n",
    "url = 'https://raw.githubusercontent.com/cedholm/M32-Gradient-descent-methods-in-machine-learning/main/Admission-Data-Rank.csv'\n",
    "dfAdmissions = pd.read_csv(url)\n",
    "\n",
    "# Print out info on datasets\n",
    "#------------------------\n",
    "\n",
    "# Prints first 5 lines of data\n",
    "print(dfAdmissions.head(5))\n",
    "\n",
    "# Print basic stats on data\n",
    "print(dfAdmissions.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a9ece",
   "metadata": {
    "id": "25e11bd5"
   },
   "source": [
    "### Plot the data\n",
    "\n",
    "Before using Gradient Descent, we will plot the data using different colors to represent the two classes.\n",
    "\n",
    "Load the data onto your workspace, and generate a scatter plot of the second column (GRE) against the third column (GPA) using a one color if they were admitted (if there is a 1 in the first column) and a different color if they were not admitted (if there is a 0 in the first column).\n",
    "\n",
    "Your plot should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf98de",
   "metadata": {
    "id": "a0d066bc",
    "outputId": "0feadb14-7095-4e79-99a8-72ade3794e3b"
   },
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Create plot\n",
    "#------------------------\n",
    "\n",
    "# Make figure size\n",
    "fig = plt.figure(dpi = 140)\n",
    "\n",
    "# Plot df data\n",
    "#------------------------\n",
    "sns.scatterplot(data = dfAdmissions, x = 'gre', y = 'gpa', hue = 'admit', style = \"admit\")\n",
    "\n",
    "# Label figure\n",
    "plt.xlabel('GRE score')\n",
    "plt.ylabel('GPA')\n",
    "plt.title('Admission by GPA and GRE scores')\n",
    "\n",
    "# Uncommment to save figure\n",
    "#------------------------\n",
    "plt.savefig('admission.pdf', bbox_inches='tight', dpi= 300)\n",
    "\n",
    "# Show the plot\n",
    "#------------------------\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2bffe",
   "metadata": {
    "id": "cb4f3b51"
   },
   "source": [
    "### The minimization problem\n",
    "\n",
    "We want to maximize the log likelihood function, so the cost function that we want to *minimize* is the negative of the log likelihood.\n",
    "\n",
    "The update rule is\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{(t+1)}=\\boldsymbol{\\theta}^{(t)}-\\gamma^{(t)}  \\nabla (- \\ell) =\\boldsymbol{\\theta}^{(t)}+\\gamma^{(t)}  \\nabla \\ell\n",
    "$$\n",
    "\n",
    "Note that, in the code, we are writing $\\boldsymbol{\\theta}$, the gradient vector $\\nabla \\ell$, and the input vector ${\\bf x}^{(i)}$, as *column* vectors.\n",
    "\n",
    "We note that since $\\nabla \\ell$ involves a sum, a large data set could result in large values of the partial derivatives.  To control the size of these entries, it is often helpful to use the *mean* log likelihood, ie. $\\frac{1}{N} \\nabla \\ell$. This is what we recommend for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d7487",
   "metadata": {
    "id": "705d69fa"
   },
   "source": [
    "### Implementation\n",
    "\n",
    "1. Let $N$ be the number of students in the dataset. Define ${\\bf y}$ as the first column of the data matrix, which is an $N \\times 1$ column vector of 0's and 1's giving the admission data.  Define $x_1$ to be the data column of GRE values and $x_2$ to be the data column of GPA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54dc81",
   "metadata": {
    "id": "25585da8"
   },
   "outputs": [],
   "source": [
    "# Name the data\n",
    "#------------------------\n",
    "y = dfAdmissions['admit'].values\n",
    "x1 = dfAdmissions['gre'].values\n",
    "x2 = dfAdmissions['gpa'].values\n",
    "\n",
    "# Number of entries in dataset\n",
    "#------------------------\n",
    "N = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d2fbb",
   "metadata": {
    "id": "48357b6e"
   },
   "source": [
    "2. Normalize each column of this input matrix (this helps the algorithm converge). One way to make the inputs more or less the same size is to subtract the mean of each column, and then divide by the standard deviation of each column:\n",
    "\n",
    "    $$x_i \\mapsto \\frac{x_i - \\bar{x}_i}{\\sigma_i}$$\n",
    "    \n",
    "    where $\\bar{x}_i$ is the mean of the $x_i$s, and $\\sigma_i$ is the standard error of the $x_i$s.\n",
    "    \n",
    "    Define ${\\bf X}$ as the $N \\times 2$ matrix consisting of the normalized $x_1$ and $x_2$ data columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978baa8",
   "metadata": {
    "id": "5aef2571"
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "#------------------------\n",
    "normalized_x1 = (x1-np.mean(x1))/np.std(x1)\n",
    "normalized_x2 = (x2-np.mean(x2))/np.std(x2)\n",
    "\n",
    "# Data to import into functions\n",
    "#------------------------\n",
    "X = np.column_stack((normalized_x1,normalized_x2)) # our normalized x_1 and x_2 data in matrix form X=[x_1 x_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc5265",
   "metadata": {
    "id": "f2bb8d54"
   },
   "source": [
    "3. Initialize the algorithm: Start with a value of $\\boldsymbol{\\theta}^0$ (one common choice for $\\boldsymbol{\\theta}^0$ is the zero vector), and set the number of iterations to 0. Fix the rate parameter, $\\gamma$, typically $\\gamma = 0.01$, the maximum number of iterations, and the convergence threshold, $\\epsilon$.  We used $\\epsilon = 10^{-6}$. In this example, we write $\\boldsymbol{\\theta}^0$ as a $1 \\times 3$ row vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f4473",
   "metadata": {
    "id": "a47e353f"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#------------------------\n",
    "\n",
    "# Starting point\n",
    "Theta = [0,0,0]\n",
    "\n",
    "# Step size to move along gradient\n",
    "gamma = 0.01\n",
    "\n",
    "# initialize num of steps to take\n",
    "totalNumSteps = 0\n",
    "# create a max number of steps to take\n",
    "maxSteps = 200000\n",
    "# how close we want to be to f\n",
    "tolerance = 0.000001\n",
    "# initial difference between old and new theta values\n",
    "diff = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5763376",
   "metadata": {
    "id": "010fda1b"
   },
   "source": [
    "4. **Main Loop:**\n",
    "\n",
    "For each value of the parameter vector, $\\boldsymbol{\\theta}$:\n",
    "- Calculate the $N \\times 1$ vector of probabilities:\n",
    "    $$p({\\bf X}, \\boldsymbol{\\theta}) = \\dfrac{1}{1 + e^{-{\\bf X} * \\boldsymbol{\\theta}}}.$$\n",
    "- The $1 \\times 3$ gradient vector can be compactly calculated as:\n",
    "    $$ \\nabla \\ell = \\Big({{\\bf y} -{ p}({\\bf X}, \\boldsymbol{\\theta})}\\Big)^T* {\\bf X}. $$\n",
    "- The next value of theta is calculated:\n",
    "    $ \\boldsymbol{\\theta}^{t+1} = \\boldsymbol{\\theta}^t + \\frac{1}{N} \\gamma \\nabla \\ell .$\n",
    "- Increment the number of iterations.\n",
    "- Test for convergence.\n",
    "\n",
    "Continue until the algorithm has converged (the values of $\\boldsymbol{\\theta}$ change by less than some threshold amount, e.g., continue until $||\\boldsymbol{\\theta}^{t+1} - \\boldsymbol{\\theta}^t|| < 10^{-6} $), or the maximum number of iterations has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72c344",
   "metadata": {
    "id": "35def0dd",
    "outputId": "065dc5c5-ca67-47fe-9112-54288c67143b"
   },
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Create gradient function\n",
    "#------------------------\n",
    "\n",
    "# Create the gradient vector\n",
    "#------------------------\n",
    "def gradf(X,y, Theta):\n",
    "\n",
    "    # Create Xtilde = column of ones merged with X\n",
    "    N = len(y)\n",
    "    X0 = np.ones((N,1))\n",
    "    # Add column of ones to create matrix Xtilde\n",
    "    Xtilde = np.column_stack((X0, X))\n",
    "\n",
    "    # Compute scores via dot product\n",
    "    scores = Xtilde@Theta # Nx1 vector\n",
    "\n",
    "    # Compute probability function\n",
    "    predictions = 1 / (1 + np.exp(-scores)) # Nx1\n",
    "\n",
    "    # Compute gradient via dot product\n",
    "    gradient = (y-predictions)@Xtilde # 1x3\n",
    "    return gradient\n",
    "\n",
    "#------------------------\n",
    "# Gradient descent calcs\n",
    "#------------------------\n",
    "\n",
    "while((totalNumSteps < maxSteps)&(diff > tolerance)):\n",
    "\n",
    "    # Calculate new (x,y) values - move along gradient to new (x,y) position\n",
    "    newTheta = Theta + 1/N*gamma*gradf(X,y,Theta)\n",
    "\n",
    "    # Calculate the difference between current and new (x,y) values\n",
    "    diff = norm(newTheta-Theta,2)\n",
    "\n",
    "    # Reassign current (x,y) to new (x,y)\n",
    "    Theta = newTheta\n",
    "\n",
    "    # Add 1 to counter for total number of steps\n",
    "    totalNumSteps += 1\n",
    "\n",
    "# Get thetas\n",
    "#------------------------\n",
    "scaled_theta0 = Theta[0]\n",
    "scaled_theta1 = Theta[1]\n",
    "scaled_theta2 = Theta[2]\n",
    "\n",
    "print('After', totalNumSteps,'timesteps, the SCALED thetas for our logistic regression model are:\\nscaled_theta0 =', scaled_theta0)\n",
    "print('scaled_theta1 =', scaled_theta1)\n",
    "print('scaled_theta2 =', scaled_theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99335d1b",
   "metadata": {
    "id": "462a3112"
   },
   "source": [
    "Once the algorithm has converged, use your value of $\\boldsymbol{\\theta}$ to draw the line that best separates the admitted points from the non-admitted points.  \n",
    "\n",
    "- If you normalized the input vectors initially, you should convert back to the original variables by transforming the estimated values of $\\boldsymbol{\\theta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_0 + \\theta_1 \\frac{x_1 - \\bar{x}_1}{\\sigma_1} + \\theta_2 \\frac{x_2 - \\bar{x}_2}{\\sigma_2} =\n",
    "\\Big({\\theta_0 - \\frac{\\theta_1}{\\bar{x}_1}{\\sigma_1} - \\frac{\\theta_2}{\\bar{x}_2}{\\sigma_2}}\\Big)  + \\frac{\\theta_1}{\\sigma_1}x_1 + \\frac{\\theta_2}{\\sigma_2}x_2\n",
    "= \\hat{\\theta_0} + \\hat{\\theta_1} x_1 + \\hat{\\theta_2} x_2\n",
    "\\end{equation}\n",
    "\n",
    "and use these transformed values, $\\hat{\\theta}_0$, $\\hat{\\theta}_1$, $\\hat{\\theta}_2$ in what follows in lieu of\n",
    "${\\theta}_0$, ${\\theta}_1$, ${\\theta}_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a79cb",
   "metadata": {
    "id": "29486395",
    "outputId": "f155d305-9da0-480a-c203-0481f2ebff5f"
   },
   "outputs": [],
   "source": [
    "# Calculuate unscaled thetas\n",
    "#------------------------\n",
    "theta0 = scaled_theta0 - scaled_theta1*np.mean(x1)/(np.std(x1)) - scaled_theta2*np.mean(x2)/(np.std(x2))\n",
    "theta1 = scaled_theta1/(np.std(x1))\n",
    "theta2 = scaled_theta2/(np.std(x2))\n",
    "\n",
    "print('After', totalNumSteps,'timesteps, the thetas for our logistic regression model are:\\ntheta0 =', theta0)\n",
    "print('theta1 =', theta1)\n",
    "print('theta2 =', theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070efb7",
   "metadata": {
    "id": "96e06956"
   },
   "source": [
    "This line is called the *decision boundary* for the classification problem, and is given as the points that would be equally likely to be admitted as not admitted:\n",
    "\n",
    "\\begin{equation}\n",
    "p({\\bf x}; \\boldsymbol{\\theta}) =  0.5\\;  \\Leftrightarrow \\;\n",
    "\\frac{1}{1 + e^{-\\boldsymbol{\\theta}^T {\\bf \\tilde{x}}}} = \\frac{1}{2} \\; \\Leftrightarrow \\;\n",
    "\\boldsymbol{\\theta}^T {\\bf \\tilde{x}} = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5b9ea",
   "metadata": {
    "id": "78fdad33",
    "outputId": "fde66ce9-6637-48ea-adff-d24754a416d6"
   },
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Create plot\n",
    "#------------------------\n",
    "\n",
    "# Create list of points to plot for the decision boundary\n",
    "#---------------------\n",
    "x_1 = np.linspace(300, 800, 100)\n",
    "x_2 = -1/theta2*(theta0 + theta1*x_1)\n",
    "\n",
    "# Make figure size\n",
    "fig = plt.figure(dpi = 140)\n",
    "\n",
    "# Plot df data and decision boundary\n",
    "#---------------------\n",
    "sns.scatterplot(data = dfAdmissions, x = 'gre', y = 'gpa', hue = 'admit', style = 'admit', legend=True)\n",
    "plt.plot(x_1, x_2, marker = '', color = 'crimson', label = 'Decision boundary')\n",
    "\n",
    "# Label figure\n",
    "plt.xlabel('GRE score')\n",
    "plt.ylabel('GPA')\n",
    "plt.title('Admission by GPA and GRE scores')\n",
    "\n",
    "# Uncommment to save figure\n",
    "#---------------------\n",
    "plt.savefig('admissionDecisionBoundary.pdf', bbox_inches='tight', dpi= 300)\n",
    "\n",
    "# Show the plot\n",
    "#---------------------\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d06fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
